{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M_Fried_M9_Assn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edl_Y5DNkyrx"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "Pick three of your favorite books on one of your favorite subjects. At least one of the books should have more than one author. For each book, include the title, authors, and two or three other attributes that you find interesting. Take the information that you’ve selected about these three books, and separately create two files, one of which stores the books’ information in HTML (using an html table) and the other of which stores the books’ information in JSON format (e.g. “books.html” and “books.json”). Post the two source files to your GitHub repository.  Write Python code, using your packages of choice,\n",
        "to load the information from each of the two files you’ve created into separate PANDAS data frames. Are the two data frames identical?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDyfuw_IoRQU"
      },
      "source": [
        "### JSON Solution\n",
        "\n",
        "For JSON, I did it the first way, then saw that it is pretty ugly, so I switched to the second way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "JHWc_eAakqjS",
        "outputId": "a7cf9dc6-1af1-469a-99a5-b9a4777b0ce5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_json = pd.read_json(\"https://raw.githubusercontent.com/MatthewFried/MAT5001/main/M9/books.json\")\n",
        "df_json.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>books</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'id': '01', 'name': 'Bava Metzia', 'edition':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'id': '02', 'name': 'Bava Kama', 'edition': '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'id': '03', 'name': 'Bava Basra', 'edition': ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               books\n",
              "0  {'id': '01', 'name': 'Bava Metzia', 'edition':...\n",
              "1  {'id': '02', 'name': 'Bava Kama', 'edition': '...\n",
              "2  {'id': '03', 'name': 'Bava Basra', 'edition': ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qu1tyBrnjHw",
        "outputId": "684fa815-5a14-44c3-95a4-25a4e3f120c7"
      },
      "source": [
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "\n",
        "import pandas as pd    \n",
        "\n",
        "request=Request('https://raw.githubusercontent.com/MatthewFried/MAT5001/main/M9/books.json')\n",
        "response = urlopen(request)\n",
        "elevations = response.read()\n",
        "data = json.loads(elevations)\n",
        "df_json_pretty = pd.json_normalize(data['books'])\n",
        "print(df_json_pretty)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id         name  edition              author category\n",
            "0  01  Bava Metzia  unknown  [Ravina, Rav Ashi]  Nezikin\n",
            "1  02    Bava Kama  unknown  [Ravina, Rav Ashi]  Nezikin\n",
            "2  03   Bava Basra  unknown  [Ravina, Rav Ashi]  Nezikin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYUeitNvpQy5"
      },
      "source": [
        "### HTML Solution\n",
        "\n",
        "The read_html function creates a list of dataframes. In this case, since I would like to use the data, I convert it to a regular dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ynTEfHr2oahw",
        "outputId": "c473cd0f-abc4-455a-e83c-7f978f18854c"
      },
      "source": [
        "df_html = pd.read_html(\"https://raw.githubusercontent.com/MatthewFried/MAT5001/main/M9/books1.html\")\n",
        "df_html = pd.concat(df_html)\n",
        "df_html.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>edition</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Bava Metzia</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Ravina and Rav Ashi</td>\n",
              "      <td>Nezikin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Bava Kama</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Ravina and Rav Ashi</td>\n",
              "      <td>Nezikin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Bava Basra</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Ravina and Rav Ashi</td>\n",
              "      <td>Nezikin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id         name  edition               author category\n",
              "0   1  Bava Metzia  unknown  Ravina and Rav Ashi  Nezikin\n",
              "1   1    Bava Kama  unknown  Ravina and Rav Ashi  Nezikin\n",
              "2   1   Bava Basra  unknown  Ravina and Rav Ashi  Nezikin"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eScAkSNSpm-A"
      },
      "source": [
        "# Part 2\n",
        "\n",
        "\n",
        "\n",
        "* Download the web page containing the Katz School’s AI Faculty\n",
        "[information](https://www.yu.edu/katz/programs/graduate/artificial-intelligence/faculty).\n",
        "\n",
        "* Create a BeautifulSoup class to parse the page you have downloaded.\n",
        "\n",
        "* Within the downloaded content of the web page locate the div with class=”body”, and assign the results to a variable named faculty.\n",
        "\n",
        "* Create a Pandas dataframe named faculty_info having columns name, title, bio, and teaching background. Each column should be capable of storing character strings.\n",
        "\n",
        "* Within the HTML content stored within your faculty variable, locate and extract each faculty member’s name, title, bio, and teaching background and save these items to your faculty_info dataframe. You should have one dataframe row for each faculty member listed on the Katz School’s AI Faculty web page. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tPcujfOEDEA"
      },
      "source": [
        "### Solution\n",
        "\n",
        "I download the data using BeautifulSoup and use a findall to assign the required data to nameList.  I note taht get_text() will get rid of all the html brackets.  I then convert the data to a list by parsing at the \\n.  Then I add the data to a named table and then put in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrhZAwizRGP"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#download data and parse with BeautifulSoup\n",
        "html = urlopen('https://www.yu.edu/katz/programs/graduate/artificial-intelligence/faculty')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "\n",
        "nameList = bs.find_all('div', {'class':'body'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at8ONIsS2JNQ",
        "outputId": "174c13ed-6c6d-482d-b74e-a96124e401e4"
      },
      "source": [
        "#pandas dataframe\n",
        "\n",
        "column_names = ['name','title','bio','teaching background']\n",
        "df = pd.DataFrame(columns=column_names)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [name, title, bio, teaching background]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srU2p1aE-F2j"
      },
      "source": [
        "#convert nameList into clean text using get_text() from BeautifulSoup\n",
        "#store values in a list\n",
        "\n",
        "test = []\n",
        "for name in nameList:\n",
        "    test.append(name.get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjoj3Nwi_9M_"
      },
      "source": [
        "#since it is stored as a 1 dimensional list that is a string, put it in a string\n",
        "my_string = test[0]\n",
        "\n",
        "#split the data on the \\n \n",
        "test2 = [x.strip() for x in my_string.split('\\n')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIxuBurBAKBm",
        "outputId": "84253307-7f07-44ca-9df2-4348b33c8ff1"
      },
      "source": [
        "#check what we have\n",
        "\n",
        "counter = 0\n",
        "for k in test2:\n",
        "  print(counter, k)\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Paul Russo, Ph.D. Vice Provost and Dean\n",
            "1 Recent research examines computational methods in social networks and mobile applications by developing and empirically evaluating causal models. Studies focus on social sharing in online communities that are influenced by user motivations, trust, and network properties. This computational social science on the web promises to improve AI algorithms on platforms like Facebook, Instagram, Google Maps, and Tinder. In earlier projects, such as those at Texas Instruments, used Pattern Analysis and Machine Intelligence methods—e.g., clustering and Horn&Schunk algorithm—for image and streaming video analysis for machine vision and tracking moving objects. Other projects have bridged engineering and physics methods to build data collection systems that identify particles emitted as deep space materials degrade. Ethnographic studies in the use of technology in distributed science collaborations. Active peer reviewer. Co-PI for a $15M award to create the CUNY Center for Big Data.\n",
            "2 Teaches graduate courses in Social Computing, Digital Media, Information Systems, Technology Entrepreneurship, Organizational Behavior, and thesis supervision.\n",
            "3 Andy Catlin, Program Director, Data Analytics and Visualization\n",
            "4 Data scientist and data system developer with expertise in financial instrument pricing and forecasting using weighted Black Scholes Merton model as well as incorporating yield curve analysis into emerging markets products. Recent projects focus on real time market feeds and server-based cascading triggers as well as migrating client-server systems into web and cloud hosted solutions. Previous projects focused on incorporating artificial intelligence and neural networks in specialized applications; C++ financial libraries; security authentication bottleneck correction; patient-days forecasting model using Box Jenkins; production replication architectures; and multi-phase commit and log-shipping architectures. Founder of multiple tech startups, including the Hudson Technology Group (acquired by Incepta), which served major clients including Fidelity Investments; Smart Money; Donaldson, Lufkin and Jenrette (DLJ); Manufacturers Hanover Trust; National Football League; and The Wall Street Journal.\n",
            "5 Teaches graduate courses in Analytics Programming, Recommendation Systems, Regression Modeling, Network Analysis, Natural Language Processing, and Neural Networks.\n",
            "6 Wonjun Lee, Assistant Professor, Artificial Intelligence\n",
            "7 Recent research focuses on designing AI-based analytics platforms that provide intelligent analysis capability across disciplines. Currently developing knowledge-based response systems for information and communication systems (ICS) inspired by human immunology. Developing AI analytics platforms in collaboration with researchers from NOAA, University of Texas at San Antonio, Auburn University, and University of Georgia. Previous research at University of Texas at San Antonio applied deep learning models, including recurrent and convolutional neural networks, to existing research areas, such as the analysis of malign network traffic and privacy issues of Internet of Things (IoT) home devices. As a principal researcher at Samsung, focused on cybersecurity in mobile app penetration and cloud computing assessment.\n",
            "8 Teaches graduate courses in Security Analytics, Neural Network and Deep Learning, Information Security, and System Programming.\n",
            "9 Sergey Fogelson, Instructor\n",
            "10 Data scientist and data product architect with expertise in media and advertising-related pricing, scheduling, and forecasting, and in large-scale anonymized identity models. Major projects have included building petabyte-scale data warehouses for media asset management and consumption analysis use-cases; fault-tolerant data products utilizing human-in-the-loop machine learning algorithms for back-office financial applications; and risk-scoring algorithms for 3rd-party cybersecurity vendor risk management. Doctoral research examined hierarchical category learning mechanisms in the visual system utilizing supervised learning algorithms applied to functional magnetic resonance imaging (fMRI) data.\n",
            "11 Teaches graduate courses in Computational Statistics, Linear Algebra, and Machine Learning.\n",
            "12 Lawrence Fulton, Instructor\n",
            "13 Health data scientist with expertise in machine learning for image recognition, especially applied to 4D MRIs, and in healthcare simulation in Java, PySim, RSimmer, and ProModel.  Current research focuses on the use of mathematical programming for improving health system performance in the areas of cost, quality, and access. Previous work focused on the application of artificial intelligence and neural networks in specialized applications, the use of Python and R applications to health data science problems, and the application of hierarchical forecasting models using both imagery and time series data simultaneously. Published in over 70 peer-reviewed journals.\n",
            "14 Teaches graduate courses in Data Analytics, Machine Learning, and Structured Data Management.\n",
            "15 Jeff Nieman, Instructor\n",
            "16 Data scientist and project manager with expertise in visualization and predictive modeling for Fortune 500 companies, including Ford and Cisco. Current work involves building systems engineering and model governance and support approaches using open source and COTS software like Alteryx, Qlik, Tableau and DataRobot. Led initiative to leverage data science optimization across complex server ecosystems, increasing server performance and minimizing risk through automated system and usage monitoring. Other key projects have included automating quoting for service renewals, tracking quoting and booking against opportunity data, enabling users to embed snapshots of visualizations in email notifications, and developing a major app for all Ford drivers.\n",
            "17 Teaches graduate courses in Predictive Modeling, Mathematics, Statistics, Machine Learning, and Project Management.\n",
            "18 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0qRqMqKA4nJ"
      },
      "source": [
        "#create tables, assign accordingly, and then put in the dataframe\n",
        "#we split at the comma so that we can get name and title from the same line\n",
        "\n",
        "name = []\n",
        "title = []\n",
        "bio = []\n",
        "teaching_background = []\n",
        "\n",
        "counter = 0\n",
        "for k in range(0,len(test2)-1,3):\n",
        "  test3 = [x.strip() for x in test2[k].split(',')]\n",
        "  name.append(test3[0])\n",
        "  title.append(test3[1])\n",
        "  bio.append(test2[k+1])\n",
        "  teaching_background.append(test2[k+2])\n",
        "\n",
        "df['name'] = name\n",
        "df['title'] = title\n",
        "df['bio'] = bio\n",
        "df['teaching background'] = teaching_background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQF69UZuC95E",
        "outputId": "8172a412-d067-40a1-f925-aa21f3fa0227"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              name  ...                                teaching background\n",
            "0       Paul Russo  ...  Teaches graduate courses in Social Computing, ...\n",
            "1      Andy Catlin  ...  Teaches graduate courses in Analytics Programm...\n",
            "2       Wonjun Lee  ...  Teaches graduate courses in Security Analytics...\n",
            "3  Sergey Fogelson  ...  Teaches graduate courses in Computational Stat...\n",
            "4  Lawrence Fulton  ...  Teaches graduate courses in Data Analytics, Ma...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7HL6RckLQ5w"
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyoXc3ywX5DB"
      },
      "source": [
        "Our job is to use data from the nytimes API, download, and analyze it.  We first register for the nytimes API and then use our key to download data.  The data is stored in JSON format.  We use typical JSON python methods such as requests, get, load, and dump.  We leave the second code block below in case we want to see the JSON data.\n",
        "\n",
        "In order to clean it better we use the third code block below and work through the data as commented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLAvJwMbLSNM",
        "outputId": "495997be-3bb6-4a89-9e2c-43e934ffbcbb"
      },
      "source": [
        "#potential request, showing that the api works with a response of 200\n",
        "\n",
        "import requests\n",
        "\n",
        "response = requests.get(\"https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json?api-key=AcIwtjAMmQbO2Zd0QNcAJ8pGtZGZAbqB&offset=40\")\n",
        "print(response)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX_it9JxLpsy"
      },
      "source": [
        "#make it readable\n",
        "#sourced from https://www.dataquest.io/blog/python-api-tutorial/\n",
        "\n",
        "import json\n",
        "\n",
        "def jprint(obj):\n",
        "    # create a formatted string of the Python JSON object\n",
        "    text = json.dumps(obj, sort_keys=True, indent=4)\n",
        "    print(text)\n",
        "\n",
        "#jprint(response.json())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1y4WwvCMOSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98289e2-f165-42fb-dd8f-383ce9c05e86"
      },
      "source": [
        "#redo request in order to make it more readable using a format that normalizes it in order to place in a dataframe\n",
        "\n",
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "\n",
        "import pandas as pd    \n",
        "\n",
        "rqst=Request(\"https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json?api-key=AcIwtjAMmQbO2Zd0QNcAJ8pGtZGZAbqB&offset=60\")\n",
        "rsp = urlopen(rqst)\n",
        "elevations = rsp.read()\n",
        "data = json.loads(elevations)\n",
        "\n",
        "print(df_books.shape)\n",
        "df_books = pd.json_normalize(data['results'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omBAHlQ9M3c4",
        "outputId": "911d7e68-9092-4c5e-c356-4d382c01c9e3"
      },
      "source": [
        "print(df_books.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             title  ...                                            reviews\n",
            "0                 12 BEAST, VOL. 3  ...  [{'book_review_link': '', 'first_chapter_link'...\n",
            "1  12 SHORT HIKES NEAR LAKE PLACID  ...  [{'book_review_link': '', 'first_chapter_link'...\n",
            "2                 12-ALARM COWBOYS  ...  [{'book_review_link': '', 'first_chapter_link'...\n",
            "3                            12.21  ...  [{'book_review_link': 'https://www.nytimes.com...\n",
            "4         1225 CHRISTMAS TREE LANE  ...  [{'book_review_link': '', 'first_chapter_link'...\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV12Gz4kNCqs",
        "outputId": "2f14ab78-9deb-4ca5-d54d-ff91864ceaa1"
      },
      "source": [
        "#we check our column names\n",
        "\n",
        "print(df_books.columns)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['title', 'description', 'contributor', 'author', 'contributor_note',\n",
            "       'price', 'age_group', 'publisher', 'isbns', 'ranks_history', 'reviews'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4B5rSauNjrm",
        "outputId": "9ada90c6-77d7-4ce8-9e45-e4e6ebb799f6"
      },
      "source": [
        "#we are doing a cursory analysis, and therefore drop anything we don't need\n",
        "\n",
        "df_books.drop(['publisher','description','contributor','contributor_note','age_group','ranks_history', 'reviews','isbns'],axis = 1,inplace=True)\n",
        "print(df_books.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             title                 author price\n",
            "0                 12 BEAST, VOL. 3                OKAYADO  0.00\n",
            "1  12 SHORT HIKES NEAR LAKE PLACID             Phil Brown  0.00\n",
            "2                 12-ALARM COWBOYS  Cora Seton and others  0.00\n",
            "3                            12.21        Dustin Thomason  0.00\n",
            "4         1225 CHRISTMAS TREE LANE        Debbie Macomber  0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ_xK0QSOD3u",
        "outputId": "db983862-52e9-4152-838f-d6c7e70425b2"
      },
      "source": [
        "#we double check we only have the columns we chose\n",
        "\n",
        "df_books.columns"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['title', 'author', 'price'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "PhJnNqnJPuCV",
        "outputId": "4dd5342d-24be-4ff4-9eb6-ca8cab99041d"
      },
      "source": [
        "#below are the statistics for our data\n",
        "\n",
        "df_books.describe()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>1225 CHRISTMAS TREE LANE</td>\n",
              "      <td>James Patterson and Maxine Paetro</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           title                             author price\n",
              "count                         20                                 20    20\n",
              "unique                        18                                 16     4\n",
              "top     1225 CHRISTMAS TREE LANE  James Patterson and Maxine Paetro  0.00\n",
              "freq                           2                                  2    16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHVVKR8YQA_i",
        "outputId": "79afce0e-dbc3-4598-f857-b8db803954c0"
      },
      "source": [
        "#we output the average price and the number of zeros that are in price\n",
        "\n",
        "price = []\n",
        "\n",
        "for k in df_books['price']:\n",
        "  price.append(float(k))\n",
        "\n",
        "#the average price is below\n",
        "import statistics \n",
        "print(\"Average price: \", statistics.mean(price))\n",
        "\n",
        "#the number of zeros\n",
        "print(\"Number of zeros: \", price.count(0))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average price:  2.996\n",
            "Number of zeros:  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN9XlFSdZgMM"
      },
      "source": [
        "I see that this data is quite useless.  There are many errors (such as missing data and zeros for prices).  Even though I changed the offset to give more books, for some reason it is not.  As such, our minimal analysis has come to an end even without any charts, since this data is essentially useless. "
      ]
    }
  ]
}